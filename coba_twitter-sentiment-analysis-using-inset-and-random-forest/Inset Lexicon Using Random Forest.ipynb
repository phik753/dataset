{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f6ee96",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e9fe77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source.Name</th>\n",
       "      <th>user_name</th>\n",
       "      <th>rating</th>\n",
       "      <th>snippet</th>\n",
       "      <th>date</th>\n",
       "      <th>iso_date</th>\n",
       "      <th>iso_date_of_last_edit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reviews_bogo.xlsx</td>\n",
       "      <td>Lucky Rivanto</td>\n",
       "      <td>5</td>\n",
       "      <td>Ok</td>\n",
       "      <td>7 months ago</td>\n",
       "      <td>2024-03-18 09:57:31</td>\n",
       "      <td>2024-03-18 09:57:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reviews_bogo.xlsx</td>\n",
       "      <td>Novita Catur putri</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice ü§ó</td>\n",
       "      <td>2 years ago</td>\n",
       "      <td>2022-07-07 11:04:13</td>\n",
       "      <td>2022-07-07 11:04:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reviews_bogo.xlsx</td>\n",
       "      <td>aditiya risky</td>\n",
       "      <td>5</td>\n",
       "      <td>very friendly service</td>\n",
       "      <td>2 years ago</td>\n",
       "      <td>2022-09-25 20:13:30</td>\n",
       "      <td>2022-09-25 20:13:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reviews_bogo.xlsx</td>\n",
       "      <td>Fita Julia</td>\n",
       "      <td>5</td>\n",
       "      <td>Very good‚ô•Ô∏è</td>\n",
       "      <td>2 years ago</td>\n",
       "      <td>2022-07-07 12:59:32</td>\n",
       "      <td>2022-07-07 12:59:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reviews_bogo.xlsx</td>\n",
       "      <td>M.choirul arif</td>\n",
       "      <td>5</td>\n",
       "      <td>Harga lmyan terjangkau, tempatnya bersih ada p...</td>\n",
       "      <td>6 months ago</td>\n",
       "      <td>2024-05-01 17:00:28</td>\n",
       "      <td>2024-05-01 17:00:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Source.Name           user_name  rating  \\\n",
       "0  reviews_bogo.xlsx       Lucky Rivanto       5   \n",
       "1  reviews_bogo.xlsx  Novita Catur putri       5   \n",
       "2  reviews_bogo.xlsx       aditiya risky       5   \n",
       "3  reviews_bogo.xlsx          Fita Julia       5   \n",
       "4  reviews_bogo.xlsx      M.choirul arif       5   \n",
       "\n",
       "                                             snippet          date  \\\n",
       "0                                                 Ok  7 months ago   \n",
       "1                                             Nice ü§ó   2 years ago   \n",
       "2                              very friendly service   2 years ago   \n",
       "3                                        Very good‚ô•Ô∏è   2 years ago   \n",
       "4  Harga lmyan terjangkau, tempatnya bersih ada p...  6 months ago   \n",
       "\n",
       "             iso_date iso_date_of_last_edit  \n",
       "0 2024-03-18 09:57:31   2024-03-18 09:57:31  \n",
       "1 2022-07-07 11:04:13   2022-07-07 11:04:13  \n",
       "2 2022-09-25 20:13:30   2022-09-25 20:13:30  \n",
       "3 2022-07-07 12:59:32   2022-07-07 12:59:32  \n",
       "4 2024-05-01 17:00:28   2024-05-01 17:00:28  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "gmaps = pd.read_excel(\"/merge_11_cabang.xlsx\")\n",
    "\n",
    "gmaps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b0ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmaps['snippet'] = gmaps['snippet'].fillna('')  # Ganti NaN dengan string kosong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6cd75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gmaps['snippet'].isnull().sum())  # Hitung jumlah nilai NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58571d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghapus karakter unicode\n",
    "def remove_unicode_characters(text):\n",
    "    if isinstance(text, str):  # Pastikan input adalah string\n",
    "        text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return text\n",
    "\n",
    "# Terapkan fungsi untuk menghapus karakter unicode pada kolom 'snippet'\n",
    "gmaps['snippet'] = gmaps['snippet'].apply(remove_unicode_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ea1561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(gmaps['snippet'].isnull().sum())  # Hitung jumlah nilai NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79734488",
   "metadata": {},
   "source": [
    "# Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c017019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result : \n",
      "\n",
      "0                                                   ok\n",
      "1                                                nice \n",
      "2                                very friendly service\n",
      "3                                            very good\n",
      "4    harga lmyan terjangkau, tempatnya bersih ada p...\n",
      "Name: Text Case Folding, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------ Case Folding --------\n",
    "# Using Series.str.lower() function on Pandas\n",
    "gmaps['Text Case Folding'] = gmaps['snippet'].str.lower()\n",
    "\n",
    "\n",
    "print('Case Folding Result : \\n')\n",
    "print(gmaps['Text Case Folding'].head(5))\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446c942",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a8b307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n",
      "0                                                 [ok]\n",
      "1                                               [nice]\n",
      "2                            [very, friendly, service]\n",
      "3                                         [very, good]\n",
      "4    [harga, lmyan, terjangkau, tempatnya, bersih, ...\n",
      "Name: Text Tokenizing, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re  # regex library\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  # import word_tokenize\n",
    "\n",
    "# Tambahkan kode ini untuk mengunduh sumber daya yang diperlukan\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# ------ Tokenizing ---------\n",
    "def remove_tweet_special(text):\n",
    "    text = str(text)\n",
    "    text = text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace('\\\\u', \" \").replace('\\\\', \"\")\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    text = ' '.join(re.sub(r\"([@#][A-Za-z0-9]+)|(\\\\w+:\\\\/\\\\/\\\\S+)\", \" \", text).split())\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "gmaps['Text Case Folding'] = gmaps['Text Case Folding'].fillna('').astype(str)\n",
    "gmaps['Text Case Folding'] = gmaps['Text Case Folding'].apply(remove_tweet_special)\n",
    "\n",
    "def remove_number(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "gmaps['Text Case Folding'] = gmaps['Text Case Folding'].apply(remove_number)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "gmaps['Text Case Folding'] = gmaps['Text Case Folding'].apply(remove_punctuation)\n",
    "\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "gmaps['Text Case Folding'] = gmaps['Text Case Folding'].apply(remove_whitespace_LT)\n",
    "\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "gmaps['Text Case Folding'] = gmaps['Text Case Folding'].apply(remove_whitespace_multiple)\n",
    "\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "gmaps['Text Case Folding'] = gmaps['Text Case Folding'].apply(remove_singl_char)\n",
    "\n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "gmaps['Text Tokenizing'] = gmaps['Text Case Folding'].apply(word_tokenize_wrapper)\n",
    "\n",
    "# Print results\n",
    "print('Tokenizing Result : \\n')\n",
    "print(gmaps['Text Tokenizing'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9721502a",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a62bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_6952\\597011201.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if row[0] not in normalizad_word_dict:\n",
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_6952\\597011201.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  normalizad_word_dict[row[0]] = row[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                 [ok]\n",
       "1                                               [nice]\n",
       "2                            [very, friendly, service]\n",
       "3                                         [very, baik]\n",
       "4    [harga, lumayan, terjangkau, tempatnya, bersih...\n",
       "Name: Text Normalization, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizad_word = pd.read_excel(\"https://raw.githubusercontent.com/phik753/dataset/master/dataset/kamus_kata_baku.xlsx\")\n",
    "\n",
    "normalizad_word_dict = {}\n",
    "\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1] \n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "gmaps['Text Normalization'] = gmaps['Text Tokenizing'].apply(normalized_term)\n",
    "\n",
    "gmaps['Text Normalization'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518cd2e4",
   "metadata": {},
   "source": [
    "## Translate kata yang mengandung bahasa inggris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfda40cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saat menerjemahkan token 'playground': The read operation timed out\n",
      "Error saat menerjemahkan token 'the': The read operation timed out\n",
      "Error saat menerjemahkan token 'food': The read operation timed out\n",
      "Error saat menerjemahkan token 'food': The read operation timed out\n",
      "Error saat menerjemahkan token 'worth': The read operation timed out\n",
      "Kolom 'Text Translating' berhasil ditambahkan:\n",
      "0                                                 [ok]\n",
      "1                                               [nice]\n",
      "2                           [very, friendly, melayani]\n",
      "3                                         [very, baik]\n",
      "4    [harga, lumayan, terjangkau, tempatnya, bersih...\n",
      "Name: Text Translating, dtype: object\n",
      "Error saat menerjemahkan teks '  , ,    ': No features in text.\n",
      "Error saat menerjemahkan teks '': No features in text.\n",
      "                                             snippet  \\\n",
      "0                                                 Ok   \n",
      "1                                              Nice    \n",
      "2                              very friendly service   \n",
      "3                                          Very good   \n",
      "4  Harga lmyan terjangkau, tempatnya bersih ada p...   \n",
      "\n",
      "                                  snippet_translated  \n",
      "0                                                 Ok  \n",
      "1                                              Nice   \n",
      "2                          layanan yang sangat ramah  \n",
      "3                                          Very good  \n",
      "4  Harga lmyan terjangkau, tempatnya bersih ada p...  \n"
     ]
    }
   ],
   "source": [
    "# ## Translate kata yang mengandung bahasa inggris\n",
    "# from googletrans import Translator\n",
    "# from langdetect import detect\n",
    "\n",
    "# # Inisialisasi Translator\n",
    "# translator = Translator()\n",
    "\n",
    "# # Fungsi untuk mendeteksi dan menerjemahkan hanya kata-kata bahasa Inggris\n",
    "# def translate_to_indonesian(tokens):\n",
    "#     translated_tokens = []\n",
    "#     for token in tokens:\n",
    "#         try:\n",
    "#             # Deteksi bahasa token\n",
    "#             if detect(token) == 'en':  # Jika token terdeteksi sebagai bahasa Inggris\n",
    "#                 translated = translator.translate(token, src='en', dest='id').text\n",
    "#                 translated_tokens.append(translated)\n",
    "#             else:\n",
    "#                 translated_tokens.append(token)  # Token bukan bahasa Inggris, tidak diterjemahkan\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error saat menerjemahkan token '{token}': {e}\")\n",
    "#             translated_tokens.append(token)  # Jika error, tetap gunakan token asli\n",
    "#     return translated_tokens\n",
    "\n",
    "# Translate kata yang mengandung bahasa Inggris\n",
    "from googletrans import Translator\n",
    "from langdetect import detect\n",
    "\n",
    "# Inisialisasi Translator\n",
    "translator = Translator()\n",
    "\n",
    "# Fungsi untuk mendeteksi dan menerjemahkan hanya kata-kata bahasa Inggris\n",
    "def translate_to_indonesian(tokens):\n",
    "    translated_tokens = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            # Deteksi bahasa token\n",
    "            if detect(token) == 'en':  # Jika token terdeteksi sebagai bahasa Inggris\n",
    "                translated = translator.translate(token, src='en', dest='id').text\n",
    "                translated_tokens.append(translated)\n",
    "            else:\n",
    "                translated_tokens.append(token)  # Token bukan bahasa Inggris, tidak diterjemahkan\n",
    "        except Exception as e:\n",
    "            print(f\"Error saat menerjemahkan token '{token}': {e}\")\n",
    "            translated_tokens.append(token)  # Jika error, tetap gunakan token asli\n",
    "    return translated_tokens\n",
    "\n",
    "# Tambahkan kolom 'Text Translating' ke DataFrame\n",
    "gmaps['Text Translating'] = gmaps['Text Normalization'].apply(translate_to_indonesian)\n",
    "\n",
    "# Pastikan kolom sudah ada\n",
    "print(\"Kolom 'Text Translating' berhasil ditambahkan:\")\n",
    "print(gmaps['Text Translating'].head())\n",
    "\n",
    "\n",
    "# Inisialisasi Translator\n",
    "translator = Translator()\n",
    "\n",
    "# Fungsi untuk mendeteksi dan menerjemahkan teks ke Bahasa Indonesia\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        # Deteksi bahasa teks\n",
    "        if detect(text) == 'en':  # Jika teks terdeteksi sebagai bahasa Inggris\n",
    "            translated = translator.translate(text, src='en', dest='id').text\n",
    "            return translated\n",
    "        else:\n",
    "            return text  # Jika bukan bahasa Inggris, tidak diterjemahkan\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat menerjemahkan teks '{text}': {e}\")\n",
    "        return text  # Jika terjadi error, kembalikan teks asli\n",
    "\n",
    "# Terapkan fungsi penerjemahan ke kolom 'snippet'\n",
    "gmaps['snippet_translated'] = gmaps['snippet'].apply(translate_text)\n",
    "\n",
    "# Simpan hasil ke file baru\n",
    "gmaps.to_csv(\"preprocessing_results_translated.csv\", index=False)\n",
    "\n",
    "# Tampilkan hasil penerjemahan\n",
    "print(gmaps[['snippet', 'snippet_translated']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2133d",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05bbe89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "------------------------\n",
      "O : o\n",
      "k : k\n",
      "N : n\n",
      "i : i\n",
      "c : c\n",
      "e : e\n",
      "  : \n",
      "l : l\n",
      "a : a\n",
      "y : y\n",
      "n : n\n",
      "g : g\n",
      "s : s\n",
      "t : t\n",
      "r : r\n",
      "m : m\n",
      "h : h\n",
      "V : v\n",
      "o : o\n",
      "d : d\n",
      "H : h\n",
      "j : j\n",
      "u : u\n",
      ", : \n",
      "p : p\n",
      "b : b\n",
      "2 : 2\n",
      "T : t\n",
      ". : \n",
      "M : m\n",
      "R : r\n",
      "- : -\n",
      "A : a\n",
      "w : w\n",
      "& : \n",
      "+ : \n",
      "E : e\n",
      "\n",
      " : \n",
      "U : u\n",
      "S : s\n",
      "D : d\n",
      "v : v\n",
      "Y : y\n",
      "z : z\n",
      "P : p\n",
      "C : c\n",
      "f : f\n",
      "! : \n",
      ": : \n",
      ") : \n",
      "\" : \n",
      "B : b\n",
      "G : g\n",
      "W : w\n",
      "K : k\n",
      "L : l\n",
      "( : \n",
      "? : \n",
      "1 : 1\n",
      "3 : 3\n",
      "5 : 5\n",
      "6 : 6\n",
      "9 : 9\n",
      "8 : 8\n",
      "0 : 0\n",
      "4 : 4\n",
      "% : \n",
      "/ : \n",
      "F : f\n",
      "J : j\n",
      "x : x\n",
      "# : \n",
      "' : \n",
      "Q : q\n",
      "I : i\n",
      "7 : 7\n",
      "q : q\n",
      "* : \n",
      "_ : \n",
      "{'O': 'o', 'k': 'k', 'N': 'n', 'i': 'i', 'c': 'c', 'e': 'e', ' ': '', 'l': 'l', 'a': 'a', 'y': 'y', 'n': 'n', 'g': 'g', 's': 's', 't': 't', 'r': 'r', 'm': 'm', 'h': 'h', 'V': 'v', 'o': 'o', 'd': 'd', 'H': 'h', 'j': 'j', 'u': 'u', ',': '', 'p': 'p', 'b': 'b', '2': '2', 'T': 't', '.': '', 'M': 'm', 'R': 'r', '-': '-', 'A': 'a', 'w': 'w', '&': '', '+': '', 'E': 'e', '\\n': '', 'U': 'u', 'S': 's', 'D': 'd', 'v': 'v', 'Y': 'y', 'z': 'z', 'P': 'p', 'C': 'c', 'f': 'f', '!': '', ':': '', ')': '', '\"': '', 'B': 'b', 'G': 'g', 'W': 'w', 'K': 'k', 'L': 'l', '(': '', '?': '', '1': '1', '3': '3', '5': '5', '6': '6', '9': '9', '8': '8', '0': '0', '4': '4', '%': '', '/': '', 'F': 'f', 'J': 'j', 'x': 'x', '#': '', \"'\": '', 'Q': 'q', 'I': 'i', '7': '7', 'q': 'q', '*': '', '_': ''}\n",
      "------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83216aef11547a691fc5b6516c3a167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/762 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil stemming:\n",
      "0                                               [o, k]\n",
      "1                                       [n, i, c, e, ]\n",
      "2    [l, a, y, a, n, a, n, , y, a, n, g, , s, a, n,...\n",
      "3                           [v, e, r, y, , g, o, o, d]\n",
      "4    [h, a, r, g, a, , l, m, y, a, n, , t, e, r, j,...\n",
      "Name: Text Stemming, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# # import Sastrawi package\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# import swifter\n",
    "\n",
    "\n",
    "# # create stemmer\n",
    "# factory = StemmerFactory()\n",
    "# stemmer = factory.create_stemmer()\n",
    "\n",
    "# # stemmed\n",
    "# def stemmed_wrapper(term):\n",
    "#     return stemmer.stem(term)\n",
    "\n",
    "# term_dict = {}\n",
    "\n",
    "# for document in gmaps['snippet_translated']:\n",
    "#     for term in document:\n",
    "#         if term not in term_dict:\n",
    "#             term_dict[term] = ' '\n",
    "            \n",
    "# print(len(term_dict))\n",
    "# print(\"------------------------\")\n",
    "\n",
    "# for term in term_dict:\n",
    "#     term_dict[term] = stemmed_wrapper(term)\n",
    "#     print(term,\":\" ,term_dict[term])\n",
    "    \n",
    "# print(term_dict)\n",
    "# print(\"------------------------\")\n",
    "\n",
    "\n",
    "# # apply stemmed term to dataframe\n",
    "# def get_stemmed_term(document):\n",
    "#     return [term_dict[term] for term in document]\n",
    "\n",
    "# gmaps['Text Stemming'] = gmaps['Text Translating'].swifter.apply(get_stemmed_term)\n",
    "# print(gmaps['Text Stemming'])\n",
    "\n",
    "\n",
    "# ===============\n",
    "# Stemming\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "# Iterasi melalui 'Text Translating'\n",
    "for document in gmaps['snippet_translated']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "\n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "# Stemming setiap kata\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    print(term, \":\", term_dict[term])\n",
    "\n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "# Apply stemming ke DataFrame\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "gmaps['Text Stemming'] = gmaps['snippet_translated'].swifter.apply(get_stemmed_term)\n",
    "\n",
    "# Cek hasilnya\n",
    "print(\"Hasil stemming:\")\n",
    "print(gmaps['Text Stemming'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413be136",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah', 'sdgkan', 'sdg', 'emg', 'sm', 'pls', 'mlu', 'ken',\n",
    "                       'allah', 'brb', 'btw', 'b/c', 'cod', 'cmiiw', 'fyi',\n",
    "                       'gg', 'ggwp', 'idk', 'ikr', 'lol', 'ootd', 'lmao', 'oot',\n",
    "                       'pap', 'otw', 'tfl', 'vc', 'ygy'])\n",
    "\n",
    "# ----------------------- add stopword from txt file ------------------------------------\n",
    "# read txt stopword using pandas\n",
    "# txt_stopword = pd.read_csv(\"https://github.com/phik753/dataset/blob/master/stopwordbahasa.txt\", names= [\"stopwords\"], header = None)\n",
    "# Use the raw link of the GitHub file\n",
    "txt_stopword_url = \"https://raw.githubusercontent.com/phik753/dataset/master/coba_twitter-sentiment-analysis-using-inset-and-random-forest/stopwordbahasa.txt\"\n",
    "\n",
    "# Read the stopword text file\n",
    "txt_stopword = pd.read_csv(txt_stopword_url, names=[\"stopwords\"], header=None)\n",
    "\n",
    "# convert stopword string to list & append additional stopword\n",
    "list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "gmaps['Text Filtering'] = gmaps['Text Stemming'].apply(stopwords_removal) \n",
    "\n",
    "\n",
    "print(gmaps['Text Filtering'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5dbc7c",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Mengecek jumlah total NaN pada kolom 'snippet'\n",
    "jumlah_kosong = gmaps['Text Filtering'].isna().sum()\n",
    "print(f\"Jumlah nilai kosong (NaN) pada kolom 'snippet': {jumlah_kosong}\")\n",
    "\n",
    "# Menghapus baris dengan nilai kosong pada kolom 'snippet'\n",
    "gmaps = gmaps.dropna(subset=['Text Filtering'])\n",
    "\n",
    "# Mengecek ulang jumlah NaN setelah penghapusan\n",
    "jumlah_kosong_setelah = gmaps['Text Filtering'].isna().sum()\n",
    "print(f\"Jumlah nilai kosong (NaN) pada kolom 'snippet' setelah penghapusan: {jumlah_kosong_setelah}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0939a2a4",
   "metadata": {},
   "source": [
    "# View Preprocessing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7827d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0acd109",
   "metadata": {},
   "source": [
    "# Save Preprocessing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5666e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps.to_csv(\"preprocessing results_3des\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c871e1",
   "metadata": {},
   "source": [
    "# Delete Unnecessary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2f0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= gmaps.drop(columns=['Text Normalization', 'Text Stemming', 'Text Translating'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfb116d",
   "metadata": {},
   "source": [
    "# Labeling Using Inset Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b741399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_positive = pd.read_excel('https://raw.githubusercontent.com/phik753/dataset/master/coba_twitter-sentiment-analysis-using-inset-and-random-forest/kamus_positive.xlsx')\n",
    "lexicon_positive_dict = {}\n",
    "for index, row in lexicon_positive.iterrows():\n",
    "    if row[0] not in lexicon_positive_dict:\n",
    "        lexicon_positive_dict[row[0]] = row[1]\n",
    "\n",
    "lexicon_negative = pd.read_excel('https://raw.githubusercontent.com/phik753/dataset/master/coba_twitter-sentiment-analysis-using-inset-and-random-forest/kamus_negative.xlsx')\n",
    "lexicon_negative_dict = {}\n",
    "for index, row in lexicon_negative.iterrows():\n",
    "    if row[0] not in lexicon_negative_dict:\n",
    "        lexicon_negative_dict[row[0]] = row[1]\n",
    "\n",
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    score = 0\n",
    "    for word in text:\n",
    "        if (word in lexicon_positive_dict):\n",
    "            score = score + lexicon_positive_dict[word]\n",
    "    for word in text:\n",
    "        if (word in lexicon_negative_dict):\n",
    "            score = score + lexicon_negative_dict[word]\n",
    "    sentimen=''\n",
    "    if (score > 0):\n",
    "        sentimen = 'Positive'\n",
    "    elif (score < 0):\n",
    "        sentimen = 'Negative'\n",
    "    else:\n",
    "        sentimen = 'Neutral'\n",
    "    return score, sentimen\n",
    "\n",
    "results = df['Text Filtering'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "results = list(zip(*results))\n",
    "df['Polarity Score'] = results[0]\n",
    "df['Indonesia Sentiment'] = results[1]\n",
    "#data['sentimen'] = results[1]\n",
    "#data\n",
    "\n",
    "df[['Text Filtering','Polarity Score','Indonesia Sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bee37f",
   "metadata": {},
   "source": [
    "## Sentiment Cumulative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1278c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "inset_counts = df['Indonesia Sentiment'].value_counts()\n",
    "inset_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter hanya data dengan sentimen negatif\n",
    "negative_sentiment_df = df[df['Indonesia Sentiment'] == 'Negative']\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Data dengan sentimen negatif:\")\n",
    "print(negative_sentiment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b506fd48",
   "metadata": {},
   "source": [
    "## Save Labeling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"sentiment results_trans_indo\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b62d5e",
   "metadata": {},
   "source": [
    "## Labeling Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(7, 5));\n",
    "g = sns.countplot(x='Indonesia Sentiment', data=df)\n",
    "\n",
    "ax=g.axes\n",
    "for p in ax.patches:\n",
    "     ax.annotate(f\"{p.get_height() * 100 / df.shape[0]:.1f}%\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "         ha='center', va='center', fontsize=10, color='black', rotation=0, xytext=(0, 5),\n",
    "         textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"sentiment results\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "new_df=df[df['Indonesia Sentiment']=='Negative']\n",
    "words = ' '.join(new_df['Text Filtering'])\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color='white',\n",
    "                      width=700, height=400, max_words = 100, collocations=False\n",
    "                     ).generate(words)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfceace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "new_df=df[df['Indonesia Sentiment']=='Positive']\n",
    "words = ' '.join(new_df['Text Filtering'])\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color='white',\n",
    "                      width=700, height=400, max_words = 100, collocations=False\n",
    "                     ).generate(words)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "new_df=df[df['Indonesia Sentiment']=='Neutral']\n",
    "words = ' '.join(new_df['Text Filtering'])\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color='white',\n",
    "                      width=700, height=400, max_words = 100, collocations=False\n",
    "                     ).generate(words)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd691b5",
   "metadata": {},
   "source": [
    "# Modeling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490a853",
   "metadata": {},
   "source": [
    "## Inset Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_inset = pd.read_csv(\"sentiment results\")\n",
    "\n",
    "df_inset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac6686",
   "metadata": {},
   "source": [
    "## Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f328509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Y (This is the value we will predict)\n",
    "X = df_inset[\"Text Filtering\"]\n",
    "y = df_inset[\"Indonesia Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(\"Train Data:\", len(X_train))\n",
    "print(\"Test Data:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef65df",
   "metadata": {},
   "source": [
    "## Implementation Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1566121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('indonesian'))\n",
    "response = count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countvectorizer = pd.DataFrame(response.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "df_countvectorizer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4034a68",
   "metadata": {},
   "source": [
    "## Tranforming the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5344836",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = count_vectorizer.transform(X_train)\n",
    "X_test  = count_vectorizer.transform(X_test)\n",
    "print(f'Inset Count Data Transformed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f90ff",
   "metadata": {},
   "source": [
    "## Evaluate Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ac787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=200, criterion='entropy', random_state=0)\n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5513217",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_predictions = text_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b75e3a",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde36c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test,count_predictions)\n",
    "cr = classification_report(y_test,count_predictions)\n",
    "rf_cv = accuracy_score(y_test,count_predictions)\n",
    "print('Confusion matrix: \\n',cm)\n",
    "print('Classification report: \\n',cr)\n",
    "print(f'Inset Random Forest Classifier on Count Vectors: {rf_cv}')\n",
    "\n",
    "# plot confusion matrix \n",
    "plt.figure(figsize=(6,4))\n",
    "sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "sns.heatmap(cm, cmap=plt.cm.Reds, annot=True, fmt='d', \n",
    "            xticklabels=sentiment_classes,\n",
    "            yticklabels=sentiment_classes)\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.ylabel('Aktual Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb44390",
   "metadata": {},
   "source": [
    "## Inset TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a46a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_inset1 = pd.read_csv(\"sentiment results\")\n",
    "\n",
    "df_inset1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6367b701",
   "metadata": {},
   "source": [
    "## Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Y (This is the value we will predict)\n",
    "X = df_inset1[\"Text Filtering\"]\n",
    "y = df_inset1[\"Indonesia Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53244d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(\"Train Data:\", len(X_train))\n",
    "print(\"Test Data:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed9158",
   "metadata": {},
   "source": [
    "## Implementation TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9cc2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('indonesian'))\n",
    "response = tfidf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d72a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_vectorizer= pd.DataFrame(response.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "df_tfidf_vectorizer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0f255",
   "metadata": {},
   "source": [
    "## Tranforming the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_vectorizer.transform(X_train)\n",
    "X_test  = tfidf_vectorizer.transform(X_test)\n",
    "print(f'Inset TF IDF Data Transformed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc393086",
   "metadata": {},
   "source": [
    "## Evaluate Model Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier1 = RandomForestClassifier(n_estimators=200, criterion='entropy', random_state=0)\n",
    "text_classifier1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_predictions = text_classifier1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058605f",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8acd67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm1 = confusion_matrix(y_test,tfidf_predictions)\n",
    "cr1 = classification_report(y_test,tfidf_predictions)\n",
    "rf_tfidf1 = accuracy_score(y_test,tfidf_predictions)\n",
    "print('Confusion matrix: \\n',cm1)\n",
    "print('Classification report: \\n',cr1)\n",
    "print(f'Inset Random Forest Classifier on TF-IDF Vectors: {rf_tfidf1}')\n",
    "\n",
    "# plot confusion matrix \n",
    "plt.figure(figsize=(6,4))\n",
    "sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "sns.heatmap(cm1, cmap=plt.cm.Reds, annot=True, fmt='d', \n",
    "            xticklabels=sentiment_classes,\n",
    "            yticklabels=sentiment_classes)\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.ylabel('Aktual Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505a055",
   "metadata": {},
   "source": [
    "## Comparison of Word Weighting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf2957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model = {'Model':['Inset Random Forest Classifier - Count Vectors',\n",
    "                  'Inset Random Forest Classifier - TFIDF Vectors',\n",
    "                 ],\n",
    "         'Accuracy Score':[rf_cv, rf_tfidf1]\n",
    "         }\n",
    "model_df = pd.DataFrame(model)\n",
    "model_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
